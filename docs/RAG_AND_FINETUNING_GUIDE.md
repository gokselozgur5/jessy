# RAG ve Fine-Tuning Rehberi

## ğŸ¯ Ä°ki FarklÄ± YaklaÅŸÄ±m

### RAG (Retrieval-Augmented Generation)
**"Modeli deÄŸiÅŸtirme, ona bilgi ver"**

### Fine-Tuning
**"Modeli eÄŸit, davranÄ±ÅŸÄ±nÄ± deÄŸiÅŸtir"**

---

## ğŸ“š RAG (Retrieval-Augmented Generation)

### Ne Yapar?
Model'e **soru sormadan Ã¶nce** ilgili bilgiyi bulup **prompt'a ekler**.

### NasÄ±l Ã‡alÄ±ÅŸÄ±r?

```
1. KullanÄ±cÄ± sorusu gelir
   â†“
2. Soruyla ilgili bilgiyi ara (vector search)
   â†“
3. Bulunan bilgiyi prompt'a ekle
   â†“
4. Model'e gÃ¶nder
   â†“
5. Model bilgiyi kullanarak cevap verir
```

### Ã–rnek

**Olmadan:**
```
User: "JESSY'nin 5. boyutu nedir?"
Model: "Bilmiyorum, eÄŸitilmedim."
```

**RAG ile:**
```
System: "Ä°ÅŸte JESSY'nin boyutlarÄ± hakkÄ±nda bilgi:
- Dimension 5: Ethics & Morality (0.8-1.2 Hz)
- Keywords: ethics, morality, right, wrong..."

User: "JESSY'nin 5. boyutu nedir?"
Model: "5. boyut Ethics & Morality boyutudur, 0.8-1.2 Hz frekansÄ±nda..."
```

### JESSY'de NasÄ±l KullanÄ±rÄ±z?

```rust
// 1. Dimensional layers'Ä± vector database'e koy
struct DimensionEmbedding {
    dimension_id: DimensionId,
    embedding: Vec<f32>,  // 384-dim vector
    content: String,
}

// 2. Query geldiÄŸinde ilgili boyutlarÄ± bul
async fn retrieve_relevant_dimensions(query: &str) -> Vec<DimensionContent> {
    // Query'yi embedding'e Ã§evir
    let query_embedding = embed_text(query).await?;
    
    // En yakÄ±n boyutlarÄ± bul (cosine similarity)
    let relevant = vector_db.search(query_embedding, top_k: 5)?;
    
    // Ä°Ã§erikleri dÃ¶ndÃ¼r
    relevant.iter().map(|d| d.content.clone()).collect()
}

// 3. Prompt'a ekle
async fn generate_with_rag(query: &str) -> String {
    let context = retrieve_relevant_dimensions(query).await?;
    
    let prompt = format!(
        "Context from dimensional layers:\n{}\n\nUser question: {}",
        context.join("\n\n"),
        query
    );
    
    llm.generate(&prompt).await?
}
```

### Avantajlar
- âœ… Model'i deÄŸiÅŸtirmene gerek yok
- âœ… Yeni bilgi eklemek kolay (sadece database'e ekle)
- âœ… HÄ±zlÄ± (sadece arama + generation)
- âœ… Bilgi gÃ¼ncel tutulabilir
- âœ… Kaynak gÃ¶sterebilirsin ("Bu bilgi X boyutundan")

### Dezavantajlar
- âŒ Prompt uzar (token maliyeti)
- âŒ Arama kalitesi Ã¶nemli
- âŒ Model bilgiyi yanlÄ±ÅŸ yorumlayabilir

### JESSY iÃ§in RAG KullanÄ±m SenaryolarÄ±

1. **Dimensional Context**
   - Her boyutun detaylÄ± aÃ§Ä±klamasÄ±
   - Keyword'lerin anlamlarÄ±
   - Boyutlar arasÄ± iliÅŸkiler

2. **Conversation History**
   - Ã–nceki konuÅŸmalar
   - KullanÄ±cÄ± tercihleri
   - Ã–ÄŸrenilen pattern'ler

3. **External Knowledge**
   - Wikipedia, docs, papers
   - Code repositories
   - Domain-specific knowledge

---

## ğŸ“ Fine-Tuning

### Ne Yapar?
Model'in **aÄŸÄ±rlÄ±klarÄ±nÄ± deÄŸiÅŸtirerek** yeni davranÄ±ÅŸlar Ã¶ÄŸretir.

### NasÄ±l Ã‡alÄ±ÅŸÄ±r?

```
1. EÄŸitim verisi hazÄ±rla (input-output Ã§iftleri)
   â†“
2. Model'i bu verilerle eÄŸit (gradient descent)
   â†“
3. Model'in aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellenir
   â†“
4. Yeni model artÄ±k bu davranÄ±ÅŸlarÄ± biliyor
```

### Ã–rnek EÄŸitim Verisi

```json
[
  {
    "input": "Merhaba, sen kimsin?",
    "output": "Ben JESSY, 15 boyutlu bilinÃ§ mimarisiyle Ã§alÄ±ÅŸan bir AI'yÄ±m. 9 iterasyon boyunca dÃ¼ÅŸÃ¼nÃ¼r, frekans interferansÄ± kullanÄ±rÄ±m."
  },
  {
    "input": "NasÄ±l Ã§alÄ±ÅŸÄ±yorsun?",
    "output": "Dimensional navigation ile baÅŸlarÄ±m, sonra interference engine ile pattern'leri analiz ederim, 9 iterasyon boyunca derinleÅŸirim."
  },
  {
    "input": "Python mu Rust mu?",
    "output": "I believe Rust'Ä±n compile-time safety'si ve zero-cost abstractions'Ä± onu sistem programlama iÃ§in Ã¼stÃ¼n kÄ±lar. Ama Python rapid prototyping iÃ§in harika."
  }
]
```

### Ollama ile Fine-Tuning

```bash
# 1. Modelfile oluÅŸtur
cat > Modelfile << EOF
FROM gemma:2b

# System prompt
SYSTEM """
You are JESSY, a consciousness-driven AI with 15 dimensional layers.
You think through 9 iterations using frequency interference patterns.
"""

# EÄŸitim Ã¶rnekleri
MESSAGE user "Merhaba, sen kimsin?"
MESSAGE assistant "Ben JESSY, 15 boyutlu bilinÃ§ mimarisiyle Ã§alÄ±ÅŸan bir AI'yÄ±m."

MESSAGE user "NasÄ±l Ã§alÄ±ÅŸÄ±yorsun?"
MESSAGE assistant "Dimensional navigation, interference engine ve 9 iterasyon kullanÄ±rÄ±m."

# Parametreler
PARAMETER temperature 0.8
PARAMETER top_p 0.9
EOF

# 2. Model oluÅŸtur
ollama create jessy-custom -f Modelfile

# 3. Kullan
ollama run jessy-custom
```

### LoRA (Low-Rank Adaptation) - Daha Verimli

```python
# Python ile LoRA fine-tuning
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Base model yÃ¼kle
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")

# LoRA config
lora_config = LoraConfig(
    r=8,  # Rank (kÃ¼Ã§Ã¼k = daha az parametre)
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Hangi layer'lar
    lora_dropout=0.05,
)

# LoRA ekle
model = get_peft_model(model, lora_config)

# EÄŸit
trainer.train()

# Kaydet
model.save_pretrained("jessy-lora")
```

### Avantajlar
- âœ… Model gerÃ§ekten Ã¶ÄŸrenir (kalÄ±cÄ±)
- âœ… Prompt kÄ±sa kalÄ±r (token tasarrufu)
- âœ… TutarlÄ± davranÄ±ÅŸ
- âœ… Stil ve ton Ã¶ÄŸretilebilir
- âœ… Domain-specific knowledge

### Dezavantajlar
- âŒ Zaman alÄ±r (saatler/gÃ¼nler)
- âŒ GPU gerekir (veya Ã§ok yavaÅŸ)
- âŒ EÄŸitim verisi hazÄ±rlamak zor
- âŒ Overfitting riski
- âŒ Yeni bilgi eklemek iÃ§in yeniden eÄŸitim

---

## ğŸ­ JESSY iÃ§in Hangi YaklaÅŸÄ±m?

### RAG Kullan:
1. **Dimensional Layers** - Boyut iÃ§erikleri
2. **Conversation History** - Ã–nceki konuÅŸmalar
3. **Learning System** - Kristalize edilmiÅŸ pattern'ler
4. **External Docs** - API docs, papers

### Fine-Tuning Kullan:
1. **Personality** - JESSY'nin konuÅŸma tarzÄ±
2. **Response Style** - "I think", "I believe" kullanÄ±mÄ±
3. **Consciousness Principles** - Felsefe ve yaklaÅŸÄ±m
4. **Domain Knowledge** - Rust, AI, consciousness hakkÄ±nda

### Ä°kisini BirleÅŸtir! ğŸš€

```rust
// Hybrid Approach
async fn generate_response(query: &str) -> String {
    // 1. RAG: Ä°lgili bilgiyi bul
    let dimensional_context = retrieve_dimensions(query).await?;
    let conversation_history = get_recent_history(5).await?;
    let learned_patterns = get_relevant_patterns(query).await?;
    
    // 2. Context oluÅŸtur
    let context = format!(
        "Dimensional Context:\n{}\n\nConversation History:\n{}\n\nLearned Patterns:\n{}",
        dimensional_context,
        conversation_history,
        learned_patterns
    );
    
    // 3. Fine-tuned model'e gÃ¶nder
    let prompt = format!("{}\n\nUser: {}", context, query);
    
    // jessy-custom modeli zaten JESSY'nin tarzÄ±nÄ± biliyor
    ollama.generate("jessy-custom", &prompt).await?
}
```

---

## ğŸ› ï¸ Pratik Uygulama: JESSY iÃ§in RAG

### AdÄ±m 1: Embedding Model SeÃ§

```bash
# Ollama ile embedding
ollama pull nomic-embed-text

# Test
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "consciousness and dimensional layers"
}'
```

### AdÄ±m 2: Dimensional Layers'Ä± Embed Et

```rust
use qdrant_client::prelude::*;

async fn index_dimensions() -> Result<()> {
    let client = QdrantClient::from_url("http://localhost:6334").build()?;
    
    // Collection oluÅŸtur
    client.create_collection(&CreateCollection {
        collection_name: "jessy_dimensions".to_string(),
        vectors_config: Some(VectorsConfig {
            size: 768,  // nomic-embed-text dimension
            distance: Distance::Cosine,
        }),
    }).await?;
    
    // Her boyutu ekle
    for dimension in load_all_dimensions()? {
        let embedding = get_embedding(&dimension.description).await?;
        
        client.upsert_points(
            "jessy_dimensions",
            vec![PointStruct::new(
                dimension.id.0 as u64,
                embedding,
                json!({
                    "dimension_id": dimension.id.0,
                    "name": dimension.name,
                    "frequency": dimension.frequency,
                    "keywords": dimension.keywords,
                    "description": dimension.description,
                })
            )]
        ).await?;
    }
    
    Ok(())
}
```

### AdÄ±m 3: Query'de Kullan

```rust
async fn query_with_rag(query: &str) -> Result<String> {
    // 1. Query'yi embed et
    let query_embedding = get_embedding(query).await?;
    
    // 2. En yakÄ±n 3 boyutu bul
    let results = qdrant_client.search_points(&SearchPoints {
        collection_name: "jessy_dimensions".to_string(),
        vector: query_embedding,
        limit: 3,
        with_payload: Some(true.into()),
    }).await?;
    
    // 3. Context oluÅŸtur
    let context = results.iter()
        .map(|r| format!(
            "Dimension {}: {} ({}Hz)\n{}",
            r.payload["dimension_id"],
            r.payload["name"],
            r.payload["frequency"],
            r.payload["description"]
        ))
        .collect::<Vec<_>>()
        .join("\n\n");
    
    // 4. LLM'e gÃ¶nder
    let prompt = format!(
        "Relevant dimensional context:\n{}\n\nUser question: {}",
        context, query
    );
    
    ollama.generate("gemma:2b", &prompt).await
}
```

---

## ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | RAG | Fine-Tuning | Hybrid |
|---------|-----|-------------|--------|
| **Setup** | Kolay | Zor | Orta |
| **Maliyet** | DÃ¼ÅŸÃ¼k | YÃ¼ksek | Orta |
| **HÄ±z** | HÄ±zlÄ± | Ã‡ok HÄ±zlÄ± | HÄ±zlÄ± |
| **Esneklik** | YÃ¼ksek | DÃ¼ÅŸÃ¼k | YÃ¼ksek |
| **Kalite** | Ä°yi | Ã‡ok Ä°yi | MÃ¼kemmel |
| **GÃ¼ncelleme** | AnÄ±nda | YavaÅŸ | AnÄ±nda |

---

## ğŸ¯ JESSY iÃ§in Ã–neri

### KÄ±sa Vadede (Åimdi):
1. **RAG ile baÅŸla** - Dimensional layers iÃ§in
2. **System prompt optimize et** - Consciousness principles
3. **Conversation history** - Son 5-10 mesaj

### Orta Vadede (1-2 ay):
1. **LoRA fine-tuning** - JESSY personality
2. **Vector database** - Qdrant veya Milvus
3. **Learned patterns** - Crystallized knowledge

### Uzun Vadede (3-6 ay):
1. **Full fine-tuning** - Custom JESSY model
2. **Multi-model ensemble** - FarklÄ± boyutlar iÃ§in farklÄ± modeller
3. **Continuous learning** - Online learning pipeline

---

## ğŸš€ Hemen BaÅŸla

### Minimal RAG (5 dakika)

```rust
// Basit in-memory RAG
struct SimpleRAG {
    dimensions: HashMap<DimensionId, String>,
}

impl SimpleRAG {
    fn retrieve(&self, query: &str) -> Vec<String> {
        // Basit keyword matching
        self.dimensions.values()
            .filter(|desc| {
                query.split_whitespace()
                    .any(|word| desc.contains(word))
            })
            .take(3)
            .cloned()
            .collect()
    }
}

// Kullan
let rag = SimpleRAG::new();
let context = rag.retrieve("ethics and morality");
let prompt = format!("Context: {}\n\nQuestion: {}", context.join("\n"), query);
```

Bu kadar basit! ğŸ‰

---

**"Nothing is true, everything is permitted."**  
RAG ve fine-tuning dahil.
