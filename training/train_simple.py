#!/usr/bin/env python3
"""
Simple JESSY Turkish Fine-Tuning

Minimal training script that works with Python 3.14 and M2 Mac.
"""

import json
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model
import time

print("ðŸ‡¹ðŸ‡· JESSY Turkish Fine-Tuning (Simple)")
print("=" * 60)

# Config
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
OUTPUT_DIR = "./jessy-turkish-simple"

print(f"\nðŸ“Š Configuration:")
print(f"   Model: {MODEL_NAME}")
print(f"   Device: {'MPS' if torch.backends.mps.is_available() else 'CPU'}")

# Load data
print(f"\nðŸ“‚ Loading data...")
with open('training/jessy_train.json', 'r') as f:
    train_data = json.load(f)

print(f"   Examples: {len(train_data)}")

# Load tokenizer and model
print(f"\nðŸ”¤ Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

device = "mps" if torch.backends.mps.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map=device,
)

print(f"   âœ… Loaded on {device}")

# Prepare training texts
print(f"\nðŸ”„ Preparing training data...")
texts = []
for ex in train_data:
    text = f"### Instruction:\n{ex['instruction']}\n\n### Response:\n{ex['output']}"
    texts.append(text)

# Tokenize
encodings = tokenizer(
    texts,
    truncation=True,
    max_length=256,
    padding="max_length",
    return_tensors="pt"
)

# Create simple dataset
class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    
    def __len__(self):
        return len(self.encodings['input_ids'])
    
    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.encodings['input_ids'][idx],
        }

train_dataset = SimpleDataset(encodings)
print(f"   âœ… Dataset ready: {len(train_dataset)} examples")

# Configure LoRA
print(f"\nðŸ”§ Configuring LoRA...")
lora_config = LoraConfig(
    r=8,  # Smaller rank for faster training
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Training args - FULL TRAINING
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=10,  # More epochs for better learning
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,  # Effective batch = 8
    learning_rate=2e-4,  # Slightly higher for faster convergence
    logging_steps=5,
    save_steps=50,
    save_total_limit=3,
    warmup_steps=20,
    fp16=False,  # MPS doesn't support fp16
    report_to="none",
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

# Train!
print(f"\nðŸš€ Starting FULL training...")
print(f"   10 epochs on 196 examples")
print(f"   This will take ~20-30 minutes")
print(f"   Watch loss decrease: target <0.5")
print()

start_time = time.time()

try:
    trainer.train()
    
    elapsed = time.time() - start_time
    print(f"\nâœ… Training complete!")
    print(f"   Time: {elapsed/60:.1f} minutes")
    
    # Save
    print(f"\nðŸ’¾ Saving model...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"   âœ… Saved to {OUTPUT_DIR}")
    print(f"\nðŸŽ‰ Done! Model ready for testing.")
    
except KeyboardInterrupt:
    print(f"\nâš ï¸  Interrupted")
except Exception as e:
    print(f"\nâŒ Error: {e}")
    import traceback
    traceback.print_exc()
