# ğŸš€ Jessy LoRA Training - HÄ±zlÄ± BaÅŸlangÄ±Ã§

## âœ… HazÄ±rlÄ±k TamamlandÄ±!

Her ÅŸey hazÄ±r, training'e baÅŸlayabilirsin.

## ğŸ“Š Durum

```
âœ… PyTorch 2.9.0 + MPS (Apple Silicon)
âœ… TÃ¼m paketler yÃ¼klÃ¼ (transformers, peft, datasets)
âœ… Training data: 3,242 Ã¶rnek
âœ… Validation data: 361 Ã¶rnek
âœ… Disk alanÄ±: 6.2 GB (yeterli)
```

## â±ï¸ SÃ¼re Tahmini

```
Epochs: 3
Steps: 606
Tahmini sÃ¼re: ~20 dakika (M2'de)
```

## ğŸ¯ Training BaÅŸlatma

### YÃ¶ntem 1: Otomatik Script (Ã–nerilen)

```bash
cd training
./start_training.sh
```

### YÃ¶ntem 2: Manuel

```bash
cd training
source venv/bin/activate
python3 train_lora_m2.py
```

## ğŸ“ Ã‡Ä±ktÄ±lar

Training tamamlandÄ±ÄŸÄ±nda:

```
training/
â”œâ”€â”€ models/jessy-lora-m2/          # Ana model
â”‚   â”œâ”€â”€ lora_adapters/              # LoRA adaptÃ¶rleri
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ adapter_model.bin
â”œâ”€â”€ checkpoints/jessy-lora/         # Ara checkpointler
â””â”€â”€ training_log.txt                # Training log'u
```

## ğŸ” Training SÄ±rasÄ±nda

Terminal'de gÃ¶receksin:
- Loss deÄŸerleri (azalmalÄ±)
- Training/Validation metrikleri
- Checkpoint kaydetme
- Tahmini kalan sÃ¼re

## âš¡ Optimizasyonlar

M2 iÃ§in optimize edilmiÅŸ:
- âœ… MPS acceleration
- âœ… KÃ¼Ã§Ã¼k batch size (2)
- âœ… Gradient accumulation (8)
- âœ… Full precision (fp32)
- âœ… LoRA rank 16

## ğŸ¯ Sonraki AdÄ±mlar

Training bittikten sonra:

1. **Test Et:**
   ```bash
   python3 test_model.py
   ```

2. **Ollama'ya Deploy Et:**
   ```bash
   # Model'i Ollama formatÄ±na Ã§evir
   python3 convert_to_ollama.py
   
   # Ollama'da Ã§alÄ±ÅŸtÄ±r
   ollama run jessy-lora
   ```

3. **KarÅŸÄ±laÅŸtÄ±r:**
   - Eski Jessy (Modelfile)
   - Yeni Jessy (LoRA)

## ğŸ› Sorun Giderme

### Training Ã§ok yavaÅŸ
```bash
# MPS kullanÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
python3 -c "import torch; print(torch.backends.mps.is_available())"
```

### Out of memory
```bash
# Batch size'Ä± azalt (train_lora_m2.py iÃ§inde)
per_device_train_batch_size: 1  # 2 yerine
```

### Dataset hatasÄ±
```bash
# Dataset formatÄ±nÄ± kontrol et
head -1 datasets/jessy_maximum_train.jsonl | python3 -m json.tool
```

## ğŸ“ˆ Beklenen SonuÃ§lar

```
Epoch 1: Loss ~2.5 â†’ ~1.8
Epoch 2: Loss ~1.8 â†’ ~1.5
Epoch 3: Loss ~1.5 â†’ ~1.3

Final: Eval loss ~1.2-1.4
```

## ğŸ‰ BaÅŸarÄ± Kriterleri

Training baÅŸarÄ±lÄ± sayÄ±lÄ±r:
- âœ… Loss azalÄ±yor
- âœ… Eval loss < 1.5
- âœ… Model kaydedildi
- âœ… Test Ã§Ä±ktÄ±larÄ± mantÄ±klÄ±

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Ä°lk kez:** Model indirilecek (~6GB), biraz zaman alÄ±r
2. **Gece:** Overnight training gerekmiyor, 20 dakika yeter
3. **Checkpoint:** Her 100 step'te kaydediliyor
4. **Log:** training_log.txt'ye kaydediliyor

## ğŸš€ Hadi BaÅŸlayalÄ±m!

```bash
cd training
./start_training.sh
```

---

**Not:** Ä°lk training'de model indirilecek. Sonraki training'ler daha hÄ±zlÄ± baÅŸlar.
