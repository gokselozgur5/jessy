# ğŸ”¨ Thor Training Guide

Thor'da JESSY'yi fine-tune etmek iÃ§in sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸ, temiz ve kolay kullanÄ±mlÄ± guide.

## ğŸ¯ Quick Start

### 1. Deploy to Thor (Ä°lk Kez)
```bash
./training/deploy_to_thor.sh
```

Bu komut:
- Training script'ini Thor'a kopyalar
- Dataset'leri transfer eder
- Python environment'Ä± hazÄ±rlar
- Her ÅŸeyi otomatik kurar

### 2. Start Training
```bash
./training/start_training_on_thor.sh
```

Bu komut:
- Training'i Thor'da background'da baÅŸlatÄ±r
- Output'u real-time gÃ¶sterir
- Ctrl+C ile monitoring'i durdurabilirsin (training devam eder)

### 3. Check Status
```bash
./training/check_thor_status.sh
```

Bu komut:
- Training'in Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol eder
- GPU kullanÄ±mÄ±nÄ± gÃ¶sterir
- Son log'larÄ± gÃ¶sterir
- Disk kullanÄ±mÄ±nÄ± gÃ¶sterir

## ğŸ“Š Training Details

### Configuration
- **Model**: Qwen2.5-3B-Instruct (3B parameters)
- **Method**: LoRA (Low-Rank Adaptation)
- **LoRA Rank**: 32 (high quality)
- **Batch Size**: 8 per device
- **Gradient Accumulation**: 4 (effective batch = 32)
- **Epochs**: 3
- **Learning Rate**: 2e-4
- **FP16**: Enabled (faster training)

### Expected Performance
- **Training Time**: ~30-45 minutes (depends on GPU)
- **GPU Memory**: ~8-12 GB
- **Dataset**: ~3,600 examples
- **Output Size**: ~2-3 GB

## ğŸ”§ Manual Commands

### SSH to Thor
```bash
sshpass -p jensen ssh jensen@192.168.88.55
```

### Start Training Manually
```bash
ssh jensen@192.168.88.55
cd /home/jensen/jessy-training
./run_training.sh
```

### Monitor Training
```bash
# Real-time monitoring
sshpass -p jensen ssh jensen@192.168.88.55 "tail -f /home/jensen/jessy-training/training.out"

# Or check logs
sshpass -p jensen ssh jensen@192.168.88.55 "tail -f /home/jensen/jessy-training/logs/training_*.log"
```

### Check GPU
```bash
sshpass -p jensen ssh jensen@192.168.88.55 "nvidia-smi"
```

### Stop Training
```bash
sshpass -p jensen ssh jensen@192.168.88.55 "pkill -f train_thor.py"
```

## ğŸ“ File Structure on Thor

```
/home/jensen/jessy-training/
â”œâ”€â”€ train_thor.py              # Main training script
â”œâ”€â”€ run_training.sh            # Wrapper script
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ training.out               # Training output
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ jessy_maximum_train.jsonl
â”‚   â””â”€â”€ jessy_maximum_val.jsonl
â”œâ”€â”€ models/
â”‚   â””â”€â”€ jessy-thor/           # Trained model (after training)
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ jessy-thor/           # Training checkpoints
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_*.log        # Detailed logs
â””â”€â”€ venv/                     # Python virtual environment
```

## ğŸ¯ Training Progress

Training'i takip etmek iÃ§in ÅŸu satÄ±rlarÄ± ara:

```
ğŸ” Environment Check
âœ… CUDA Available: 13.0
âœ… GPU 0: NVIDIA Thor

ğŸ“¦ Loading Model: Qwen/Qwen2.5-3B-Instruct
âœ… Tokenizer loaded
âœ… Base model loaded

ğŸ”§ Applying LoRA Configuration
ğŸ“Š Trainable Parameters: 41,943,040 (1.32%)
ğŸ“Š Total Parameters: 3,174,748,160

ğŸ“š Preparing Datasets
âœ… Loaded 3,243 training examples
âœ… Loaded 360 validation examples

ğŸš€ Starting Training
ğŸ“Š Epochs: 3
ğŸ“Š Batch Size: 8
ğŸ“Š Effective Batch Size: 32

ğŸ¯ Training Started!
[Training progress will show here...]

âœ… Training Complete!
â±ï¸  Training Time: 35.2 minutes
ğŸ“Š Best Eval Loss: 0.4523
```

## ğŸ‰ After Training

Training bitince model ÅŸurada olacak:
```
/home/jensen/jessy-training/models/jessy-thor/
```

### Download Model
```bash
# Download trained model
sshpass -p jensen scp -r jensen@192.168.88.55:/home/jensen/jessy-training/models/jessy-thor ./training/models/

# Download LoRA adapters
sshpass -p jensen scp -r jensen@192.168.88.55:/home/jensen/jessy-training/models/jessy-thor/lora_adapters ./training/models/
```

### Convert to GGUF (for Ollama)
```bash
# TODO: Add conversion script
# This will convert the model to GGUF format for Ollama
```

## ğŸ› Troubleshooting

### Training Not Starting
```bash
# Check if Python packages are installed
sshpass -p jensen ssh jensen@192.168.88.55 "cd /home/jensen/jessy-training && source venv/bin/activate && pip list | grep torch"

# Reinstall if needed
sshpass -p jensen ssh jensen@192.168.88.55 "cd /home/jensen/jessy-training && source venv/bin/activate && pip install -r requirements.txt"
```

### Out of Memory
```bash
# Edit training script to reduce batch size
# Change per_device_train_batch_size from 8 to 4
```

### CUDA Not Found
```bash
# Check CUDA installation
sshpass -p jensen ssh jensen@192.168.88.55 "nvcc --version"
sshpass -p jensen ssh jensen@192.168.88.55 "python3 -c 'import torch; print(torch.cuda.is_available())'"
```

## ğŸ“Š Monitoring Tips

### Watch GPU Usage
```bash
watch -n 1 'sshpass -p jensen ssh jensen@192.168.88.55 "nvidia-smi"'
```

### Watch Training Progress
```bash
sshpass -p jensen ssh jensen@192.168.88.55 "tail -f /home/jensen/jessy-training/training.out | grep -E '(loss|epoch|step)'"
```

### Check Disk Space
```bash
sshpass -p jensen ssh jensen@192.168.88.55 "df -h /home/jensen/jessy-training"
```

## ğŸ¯ Next Steps

1. âœ… Deploy to Thor
2. âœ… Start training
3. â³ Wait 30-45 minutes
4. âœ… Download trained model
5. ğŸ”„ Convert to GGUF
6. ğŸš€ Deploy to Ollama
7. ğŸ‰ Test JESSY!

---

**"Nothing is true, everything is permitted."**  
Including training AI on a machine named after a Norse god. âš¡ğŸ”¨

