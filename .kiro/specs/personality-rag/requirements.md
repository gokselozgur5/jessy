# Requirements Document - Personality RAG System

## Introduction

Jessy needs to embody Kiro and Göksel's personalities dynamically by retrieving relevant personality context from their steering files (realkiro.md and gokselclaude.md) using RAG (Retrieval-Augmented Generation). Instead of loading entire files into context, the system will:
1. Chunk personality files semantically
2. Generate embeddings using existing mmap-optimized embedding model
3. Store in vector database (Qdrant)
4. Retrieve relevant chunks based on user query
5. Inject into system prompt dynamically

## Glossary

- **RAG**: Retrieval-Augmented Generation - technique to retrieve relevant context before LLM generation
- **Chunk**: Semantic segment of personality file (e.g., a section, heuristic, or case study)
- **Embedding**: Vector representation of text (generated by embedding model)
- **Vector Database**: Database optimized for similarity search (Qdrant)
- **Personality Source**: Origin of personality data (Kiro or Göksel)
- **Orchestrator**: Jessy's cognitive orchestrator with mmap-optimized embedding model
- **System Prompt**: Initial context sent to Claude API before user message

## Requirements

### Requirement 1: Semantic Chunking

**User Story:** As Jessy, I want personality files to be chunked semantically so that each chunk represents a coherent concept (heuristic, framework, case study, etc.)

#### Acceptance Criteria

1. WHEN personality file is loaded, THE Chunking System SHALL parse markdown by section headers (##, ###)
2. WHEN section is larger than 500 tokens, THE Chunking System SHALL split into sub-chunks with 50-token overlap
3. WHEN chunk is created, THE Chunking System SHALL detect category (CoreIdentity, DecisionFramework, Heuristic, CommunicationPattern, PhilosophicalFramework, CaseStudy, Example)
4. WHEN chunk is created, THE Chunking System SHALL extract relevance tags from content (e.g., "dating", "decision-making", "philosophy")
5. WHEN chunk is created, THE Chunking System SHALL assign priority (1-10) based on category importance

### Requirement 2: Embedding Generation

**User Story:** As Jessy, I want to use the existing mmap-optimized embedding model in the orchestrator so that I don't need external API calls or additional dependencies

#### Acceptance Criteria

1. WHEN chunk is created, THE RAG System SHALL request embedding from CognitiveOrchestrator
2. WHEN embedding is requested, THE CognitiveOrchestrator SHALL use existing mmap-loaded embedding model
3. WHEN embedding is generated, THE RAG System SHALL store embedding vector with chunk metadata
4. WHEN batch embedding is needed, THE RAG System SHALL process chunks in parallel for performance

### Requirement 3: Vector Database Storage

**User Story:** As Jessy, I want chunks and embeddings stored in Qdrant vector database so that I can perform fast similarity search at runtime

#### Acceptance Criteria

1. WHEN RAG System initializes, THE Vector Store SHALL create Qdrant collection "jessy_personality" if not exists
2. WHEN chunk with embedding is ready, THE Vector Store SHALL upsert to Qdrant with payload (id, source, section, content, metadata)
3. WHEN collection is created, THE Vector Store SHALL configure cosine distance metric for similarity
4. WHEN embedding dimension is determined, THE Vector Store SHALL match orchestrator's embedding model dimension

### Requirement 4: Similarity Search

**User Story:** As Jessy, I want to retrieve top-K most relevant personality chunks based on user query so that I can inject relevant context into system prompt

#### Acceptance Criteria

1. WHEN user message arrives, THE RAG System SHALL generate query embedding using orchestrator
2. WHEN query embedding is ready, THE Vector Store SHALL search Qdrant for top-K similar chunks (K=5 default)
3. WHEN search completes, THE Vector Store SHALL return chunks with similarity scores
4. WHEN chunks are retrieved, THE RAG System SHALL format chunks for system prompt injection

### Requirement 5: System Prompt Injection

**User Story:** As Jessy, I want retrieved personality chunks injected into system prompt so that Claude API receives relevant context for response generation

#### Acceptance Criteria

1. WHEN relevant chunks are retrieved, THE LLM Service SHALL format chunks as "# Relevant Personality Context" section
2. WHEN system prompt is built, THE LLM Service SHALL append personality context after base Jessy prompt
3. WHEN context is injected, THE LLM Service SHALL include chunk source (Kiro or Göksel) and section title
4. WHEN token limit is approached, THE LLM Service SHALL truncate lower-priority chunks first

### Requirement 6: Initialization Script

**User Story:** As a developer, I want a one-time initialization script to chunk personality files and populate vector database so that RAG system is ready for runtime queries

#### Acceptance Criteria

1. WHEN initialization script runs, THE Script SHALL read realkiro.md and gokselclaude.md from ~/.kiro/steering/
2. WHEN files are read, THE Script SHALL chunk both files using PersonalityChunker
3. WHEN chunks are created, THE Script SHALL generate embeddings using orchestrator
4. WHEN embeddings are ready, THE Script SHALL upsert all chunks to Qdrant
5. WHEN initialization completes, THE Script SHALL log total chunks indexed and success status

### Requirement 7: Runtime Integration

**User Story:** As Jessy, I want RAG system integrated into existing LLM service so that every user message triggers relevant personality retrieval

#### Acceptance Criteria

1. WHEN LLM Service receives user message, THE Service SHALL call PersonalityRAG.retrieve_relevant_context()
2. WHEN context is retrieved, THE Service SHALL inject into system prompt before Claude API call
3. WHEN Claude API responds, THE Service SHALL return response to user
4. WHEN RAG retrieval fails, THE Service SHALL fallback to base prompt without personality context (graceful degradation)

### Requirement 8: Configuration

**User Story:** As a developer, I want RAG system configurable via environment variables so that I can tune parameters without code changes

#### Acceptance Criteria

1. WHEN RAG System initializes, THE System SHALL read QDRANT_URL from environment (default: "http://localhost:6334")
2. WHEN RAG System initializes, THE System SHALL read RAG_TOP_K from environment (default: 5)
3. WHEN RAG System initializes, THE System SHALL read RAG_CHUNK_SIZE from environment (default: 500 tokens)
4. WHEN RAG System initializes, THE System SHALL read RAG_CHUNK_OVERLAP from environment (default: 50 tokens)

### Requirement 9: Observability

**User Story:** As a developer, I want RAG system to log retrieval operations so that I can debug and monitor personality context injection

#### Acceptance Criteria

1. WHEN chunks are retrieved, THE RAG System SHALL log query, top-K chunks retrieved, and similarity scores
2. WHEN embedding generation occurs, THE RAG System SHALL log chunk count and processing time
3. WHEN vector search occurs, THE RAG System SHALL log search latency
4. WHEN initialization runs, THE RAG System SHALL log total chunks indexed per source (Kiro, Göksel)

### Requirement 10: Error Handling

**User Story:** As Jessy, I want RAG system to handle errors gracefully so that personality retrieval failures don't crash the service

#### Acceptance Criteria

1. WHEN Qdrant is unavailable, THE RAG System SHALL log error and return empty context (fallback to base prompt)
2. WHEN embedding generation fails, THE RAG System SHALL log error and skip that chunk
3. WHEN personality file is missing, THE Initialization Script SHALL log warning and continue with available files
4. WHEN vector search times out, THE RAG System SHALL return empty context after 5-second timeout
